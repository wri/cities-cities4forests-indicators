{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910a66a1-a3b7-4aee-98ab-2b826a1b6e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install pip earthengine-api\n",
    "# !{sys.executable} -m pip install pip geemap\n",
    "# !{sys.executable} -m pip install pip rasterstats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9f259e-62a6-4b7e-b879-8628b18a8741",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "# ee.Authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73901903-5245-4507-950b-3a78b2e820b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff69f1a8-e7fa-40ac-81db-f9a67f1e82c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "import boto3\n",
    "import geopandas as gpd\n",
    "import io\n",
    "from rasterstats import zonal_stats\n",
    "import fiona\n",
    "import rasterio.mask\n",
    "import geemap\n",
    "import glob\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9de7d4d-ed34-493b-8573-ab00825ca668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ea530a-3c18-423a-a28a-de82cfa137ff",
   "metadata": {},
   "source": [
    "# Read input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70af5dac-1af1-4eef-8cd0-9ea71b42829c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read Land use land cover dataset\n",
    "WC = ee.ImageCollection(\"ESA/WorldCover/v100\")\n",
    "WorldCover = WC.first();\n",
    "\n",
    "## define projection for use later\n",
    "WCprojection = WC.first().projection();  \n",
    "print('WorldCover projection:', WCprojection.getInfo());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330baf82-21ca-423c-803a-37782d5d7357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define directory\n",
    "out_dir = os.getcwd()\n",
    "aws_s3_dir = \"https://cities-cities4forests.s3.eu-west-3.amazonaws.com/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fa5ee9-8643-433d-9275-0fbc3ba0230b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of c4f cities\n",
    "boundary_georef = pd.read_csv('https://cities-cities4forests.s3.eu-west-3.amazonaws.com/data/boundaries/v_0/boundary_georef.csv')\n",
    "boundary_georef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa076655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # remove timeout cities \n",
    "# timeout_cities = ['ETH-Dire_Dawa','COD-Uvira','MEX-Mexico_City','MEX-Monterrey']\n",
    "# boundary_georef = boundary_georef[~boundary_georef['geo_name'].isin(timeout_cities)].reset_index(drop=True)\n",
    "# boundary_georef"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e1c571-5f1e-4a24-8dd7-9222ffe1097a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Compute indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2155f1c4-4efe-4b67-916f-29090041d975",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_indicators_GRE_1_3 = pd.DataFrame() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb32f45-c4ab-40b6-b0ea-a0e04827d126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters for albedo calculations: date range of interest, image limit, scale, and albedo threshold\n",
    "\n",
    "date_start = '2021-01-01'\n",
    "date_end = '2022-01-01'\n",
    "image_limit = 50 # max number of images to include, sorted from least to most cloudy\n",
    "scale = 30 # scale in meters at which to complete reductions - 10 by default, increase if experiencing timeouts with large geographies\n",
    "\n",
    "# define \"low albedo\" threshold\n",
    "LowAlbedoMax = 0.20 # EnergyStar steep slope minimum initial value is 0.25. 3-year value is 0.15. https://www.energystar.gov/products/building_products/roof_products/key_product_criteria\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84588285-a09d-427a-967c-1c8388487dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Configure methods\n",
    "\n",
    "# Read relevant Sentinel-2 data\n",
    "S2 = ee.ImageCollection(\"COPERNICUS/S2_SR\")\n",
    "S2C = ee.ImageCollection(\"COPERNICUS/S2_CLOUD_PROBABILITY\")\n",
    "\n",
    "MAX_CLOUD_PROB=30\n",
    "S2_ALBEDO_EQN='((B*Bw)+(G*Gw)+(R*Rw)+(NIR*NIRw)+(SWIR1*SWIR1w)+(SWIR2*SWIR2w))'\n",
    "S2_VIZ = {'bands': ['B4', 'B3', 'B2'], 'min': 0, 'max': 0.3};\n",
    "\n",
    "\n",
    "## METHODS\n",
    "\n",
    "## get cloudmasked image collection \n",
    "\n",
    "def mask_and_count_clouds(s2wc,geom):\n",
    "    s2wc=ee.Image(s2wc)\n",
    "    geom=ee.Geometry(geom.geometry())\n",
    "    is_cloud=ee.Image(s2wc.get('cloud_mask')).gt(MAX_CLOUD_PROB).rename('is_cloud')\n",
    "    nb_cloudy_pixels=is_cloud.reduceRegion(\n",
    "        reducer=ee.Reducer.sum().unweighted(), \n",
    "        geometry=geom, \n",
    "        scale=10, \n",
    "        maxPixels=1e9\n",
    "   )\n",
    "    return s2wc.updateMask(is_cloud.eq(0)).set('nb_cloudy_pixels',nb_cloudy_pixels.getNumber('is_cloud')).divide(10000)\n",
    "\n",
    "def mask_clouds_and_rescale(im):\n",
    "    clouds=ee.Image(im.get('cloud_mask')).select('probability')\n",
    "    return im.updateMask(clouds.lt(MAX_CLOUD_PROB)).divide(10000)\n",
    "\n",
    "def get_masked_s2_collection(roi,start,end):\n",
    "    criteria=(ee.Filter.And(\n",
    "            ee.Filter.date(start,end),\n",
    "            ee.Filter.bounds(roi)\n",
    "        ))\n",
    "    s2=S2.filter(criteria)#.select('B2','B3','B4','B8','B11','B12')\n",
    "    s2c=S2C.filter(criteria)\n",
    "    s2_with_clouds=(ee.Join.saveFirst('cloud_mask').apply(**{\n",
    "        'primary': ee.ImageCollection(s2),\n",
    "        'secondary': ee.ImageCollection(s2c),\n",
    "        'condition': ee.Filter.equals(**{'leftField':'system:index','rightField':'system:index'}) \n",
    "        }))\n",
    "    def _mcc(im):\n",
    "        return mask_and_count_clouds(im,roi) \n",
    "    #s2_with_clouds=ee.ImageCollection(s2_with_clouds).map(_mcc)\n",
    "    #s2_with_clouds=s2_with_clouds.limit(image_limit,'nb_cloudy_pixels')\n",
    "    s2_with_clouds=ee.ImageCollection(s2_with_clouds).map(mask_clouds_and_rescale)#.limit(image_limit,'CLOUDY_PIXEL_PERCENTAGE')\n",
    "    return  ee.ImageCollection(s2_with_clouds)\n",
    "\n",
    "# calculate albedo for images\n",
    "\n",
    "# weights derived from \n",
    "# S. Bonafoni and A. Sekertekin, \"Albedo Retrieval From Sentinel-2 by New Narrow-to-Broadband Conversion Coefficients,\" in IEEE Geoscience and Remote Sensing Letters, vol. 17, no. 9, pp. 1618-1622, Sept. 2020, doi: 10.1109/LGRS.2020.2967085.\n",
    "def calc_s2_albedo(image):\n",
    "    config={\n",
    "    'Bw':0.2266,\n",
    "    'Gw':0.1236,\n",
    "    'Rw':0.1573,\n",
    "    'NIRw':0.3417,\n",
    "    'SWIR1w':0.1170,\n",
    "    'SWIR2w':0.0338,\n",
    "    'B':image.select('B2'),\n",
    "    'G':image.select('B3'),\n",
    "    'R':image.select('B4'),\n",
    "    'NIR':image.select('B8'),\n",
    "    'SWIR1':image.select('B11'),\n",
    "    'SWIR2':image.select('B12')\n",
    "  }\n",
    "    return image.expression(S2_ALBEDO_EQN,config).float().rename('albedo')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fed9dec-553f-4a2e-96f6-34f1ed0644a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(boundary_georef)):\n",
    "    print(i)\n",
    "    geo_name = boundary_georef.loc[i, 'geo_name']\n",
    "    print(\"\\n geo_name: \"+geo_name)\n",
    "    \n",
    "    boundary_id_aoi = boundary_georef.loc[i, 'geo_name']+'-'+boundary_georef.loc[i, 'aoi_boundary_name']\n",
    "    boundary_id_unit = boundary_georef.loc[i, 'geo_name']+'-'+boundary_georef.loc[i, 'units_boundary_name']\n",
    "\n",
    "        \n",
    "    # process aoi level ------\n",
    "    print(\"\\n boundary_id_aoi: \"+boundary_id_aoi)\n",
    "    # read boundaries\n",
    "    boundary_path = aws_s3_dir +'/boundaries/v_0/boundary-'+boundary_id_aoi+'.geojson'\n",
    "    boundary_geo = requests.get(boundary_path).json()\n",
    "    boundary_geo_ee = geemap.geojson_to_ee(boundary_geo)\n",
    "      \n",
    "    ## S2 MOSAIC AND ALBEDO\n",
    "    dataset = get_masked_s2_collection(boundary_geo_ee,date_start,date_end)\n",
    "    s2_albedo = dataset.map(calc_s2_albedo)\n",
    "    mosaic=dataset.mean()\n",
    "    albedoMean=s2_albedo.reduce(ee.Reducer.mean())\n",
    "    albedoMeanThres = albedoMean.updateMask(albedoMean.lt(LowAlbedoMax))\n",
    "    \n",
    "\n",
    "    # define images and functions\n",
    "\n",
    "    ## function to create image of means of toCount for each asClass\n",
    "    def getmeanbyclass(classvalue):\n",
    "        return ee.Image(toCount.updateMask(asClass.eq(classvalue)) #.And(toCount.gt(0))) # uncomment And statement if you want include only pixels that meet both criteria\n",
    "                        # .unmask(0) # uncomment if you want to include all pixels not just pixels of classvalue\n",
    "                        ).rename(ee.String('') #'class_count-'\n",
    "                                           .cat(ee.Number(classvalue).toInt().format()))\n",
    "\n",
    "    ## function to create image of count of each asClass\n",
    "    def getcountbyclass(classvalue):\n",
    "        return ee.Image(toCount.updateMask(asClass.eq(classvalue)) #.And(toCount.gt(0))) # uncomment And statement if you want include only pixels that meet both criteria\n",
    "                        # .unmask(0) # uncomment if you want to include all pixels not just pixels of classvalue\n",
    "                        ).rename(ee.String('class_count-') #'class_count-'\n",
    "                                          .cat(ee.Number(classvalue).toInt().format()))\n",
    "\n",
    "    ## function to create image of count of each asClass filtered by toCount\n",
    "    def getcountbyclassFilt(classvalue):\n",
    "        return ee.Image(toCount.updateMask(asClass.eq(classvalue).And(toCount.gte(0))) # uncomment And statement if you want include only pixels that meet both criteria\n",
    "                        # .unmask(0) # uncomment if you want to include all pixels not just pixels of classvalue\n",
    "                        ).rename(ee.String('class_countFilt-') #'class_count-'\n",
    "                                          .cat(ee.Number(classvalue).toInt().format()))\n",
    "\n",
    "    ## create image with each WorldCover class mean as a band\n",
    "\n",
    "    asClass = WorldCover\n",
    "    toCount = albedoMean \n",
    "\n",
    "    # meanbyclass=ee.Image(getmeanbyclass(10)).addBands([\n",
    "    #   getmeanbyclass(20),  \n",
    "    #   getmeanbyclass(30),  \n",
    "    #   getmeanbyclass(40),  \n",
    "    #   getmeanbyclass(50),  \n",
    "    #   getmeanbyclass(60),\n",
    "    #   getmeanbyclass(70),  \n",
    "    #   getmeanbyclass(80), \n",
    "    #   getmeanbyclass(90), \n",
    "    #   getmeanbyclass(95), \n",
    "    #   getmeanbyclass(100), \n",
    "    # ])\n",
    "\n",
    "    ## create image with each WorldCover class count as a band\n",
    "\n",
    "    countbyclass=ee.Image(getcountbyclass(10)).addBands([\n",
    "      getcountbyclass(20),  \n",
    "      getcountbyclass(30),  \n",
    "      getcountbyclass(40),  \n",
    "      getcountbyclass(50),  \n",
    "      getcountbyclass(60),\n",
    "      getcountbyclass(70),  \n",
    "      getcountbyclass(80), \n",
    "      getcountbyclass(90), \n",
    "      getcountbyclass(95), \n",
    "      getcountbyclass(100), \n",
    "    ])\n",
    "\n",
    "    ## create image with each WorldCover class count above threshold as a band\n",
    "    toCount = albedoMeanThres \n",
    "\n",
    "    countbyclassFilt=ee.Image(getcountbyclassFilt(10)).addBands([\n",
    "      getcountbyclassFilt(20),  \n",
    "      getcountbyclassFilt(30),  \n",
    "      getcountbyclassFilt(40),  \n",
    "      getcountbyclassFilt(50),  \n",
    "      getcountbyclassFilt(60),\n",
    "      getcountbyclassFilt(70),  \n",
    "      getcountbyclassFilt(80), \n",
    "      getcountbyclassFilt(90),\n",
    "      getcountbyclassFilt(95), \n",
    "      getcountbyclassFilt(100), \n",
    "    ])\n",
    "\n",
    "            \n",
    "    def reducers(FC):\n",
    "        ## create FeatureCollection with mean of count for each class for each feature\n",
    "\n",
    "        histo=countbyclass.reduceRegions(\n",
    "          reducer= ee.Reducer.count(), \n",
    "          collection= FC, \n",
    "          scale= scale, \n",
    "          tileScale= 4\n",
    "        )\n",
    "\n",
    "        histo=countbyclassFilt.reduceRegions(\n",
    "          reducer= ee.Reducer.count(), \n",
    "          collection= histo, \n",
    "          scale= scale, \n",
    "          tileScale= 4\n",
    "        )\n",
    "        \n",
    "        # histo=meanbyclass.reduceRegions(\n",
    "        #   reducer= ee.Reducer.mean(), \n",
    "        #   collection= histo, \n",
    "        #   scale= 10, \n",
    "        #   tileScale= 4\n",
    "        # )\n",
    "        return histo\n",
    "    \n",
    "    \n",
    "    ## Define function to normalize count as percent of all pixels in each feature and create new properties with the values\n",
    "    \n",
    "    def count_to_percent(feat):\n",
    "        feat=ee.Feature(feat)\n",
    "        \n",
    "#         hist=ee.Dictionary(feat.toDictionary(['10','20','30','40','50','60','70','80','90','95','100']))\n",
    "#         hist=hist.set('10',hist.get('10',0))\n",
    "#         hist=hist.set('20',hist.get('20',0))\n",
    "#         hist=hist.set('30',hist.get('30',0))\n",
    "#         hist=hist.set('40',hist.get('40',0))\n",
    "#         hist=hist.set('50',hist.get('50',0))\n",
    "#         hist=hist.set('60',hist.get('60',0))\n",
    "#         hist=hist.set('70',hist.get('70',0))\n",
    "#         hist=hist.set('80',hist.get('80',0))\n",
    "#         hist=hist.set('90',hist.get('90',0))\n",
    "#         hist=hist.set('95',hist.get('95',0))\n",
    "#         hist=hist.set('100',hist.get('100',0))\n",
    "\n",
    "#         def pct_hist(k,v):\n",
    "#             # convert whole number (0-100) to decimal percent (0-1)\n",
    "#             return ee.Number(v)\n",
    "\n",
    "#         meansLULC = hist.map(pct_hist)\n",
    "\n",
    "        histC=ee.Dictionary(feat.toDictionary(['class_count-10','class_count-20','class_count-30','class_count-40','class_count-50','class_count-60','class_count-70','class_count-80','class_count-90','class_count-95','class_count-100']))\n",
    "        histC=histC.set('50',histC.get('class_count-50',0))\n",
    "        # histC=histC.set('10',histC.get('class_count-10',0))\n",
    "        # histC=histC.set('20',histC.get('class_count-20',0))\n",
    "        # histC=histC.set('30',histC.get('class_count-30',0))\n",
    "        # histC=histC.set('40',histC.get('class_count-40',0))\n",
    "        # histC=histC.set('60',histC.get('class_count-60',0))\n",
    "        # histC=histC.set('70',histC.get('class_count-70',0))\n",
    "        # histC=histC.set('80',histC.get('class_count-80',0))\n",
    "        # histC=histC.set('90',histC.get('class_count-90',0))\n",
    "        # histC=histC.set('95',histC.get('class_count-95',0))\n",
    "        # histC=histC.set('100',histC.get('class_count-100',0))\n",
    "\n",
    "        histCfilt=ee.Dictionary(feat.toDictionary(['class_countFilt-10','class_countFilt-20','class_countFilt-30','class_countFilt-40','class_countFilt-50','class_countFilt-60','class_countFilt-70','class_countFilt-80','class_countFilt-90','class_countFilt-95','class_countFilt-100']))\n",
    "        histCfilt=histCfilt.set('50',histCfilt.get('class_countFilt-50',0))\n",
    "        # histCfilt=histCfilt.set('10',histCfilt.get('class_countFilt-10',0))\n",
    "        # histCfilt=histCfilt.set('20',histCfilt.get('class_countFilt-20',0))\n",
    "        # histCfilt=histCfilt.set('30',histCfilt.get('class_countFilt-30',0))\n",
    "        # histCfilt=histCfilt.set('40',histCfilt.get('class_countFilt-40',0))\n",
    "        # histCfilt=histCfilt.set('60',histCfilt.get('class_countFilt-60',0))\n",
    "        # histCfilt=histCfilt.set('70',histCfilt.get('class_countFilt-70',0))\n",
    "        # histCfilt=histCfilt.set('80',histCfilt.get('class_countFilt-80',0))\n",
    "        # histCfilt=histCfilt.set('90',histCfilt.get('class_countFilt-90',0))\n",
    "        # histCfilt=histCfilt.set('95',histCfilt.get('class_countFilt-95',0))\n",
    "        # histCfilt=histCfilt.set('100',histCfilt.get('class_countFilt-100',0))\n",
    "\n",
    "        def area_hist(k,v):\n",
    "            # convert 10m pixel count of class to KM2 of class\n",
    "            return ee.Number(v).multiply(ee.Number(100)).multiply(ee.Number(0.000001))\n",
    "\n",
    "        classAreas = histC.map(area_hist)\n",
    "        classFiltAreas = histCfilt.map(area_hist)\n",
    "\n",
    "        # totalPixels=hist.values()\n",
    "\n",
    "        return feat.set({\n",
    "            # 'LC10albedo': meansLULC.getNumber('10'),\n",
    "            # 'LC20albedo': meansLULC.getNumber('20'),\n",
    "            # 'LC30albedo': meansLULC.getNumber('30'),\n",
    "            # 'LC40albedo': meansLULC.getNumber('40'),\n",
    "            # 'LC50albedo': meansLULC.getNumber('50'),\n",
    "            # 'LC60albedo': meansLULC.getNumber('60'),\n",
    "            # 'LC70albedo': meansLULC.getNumber('70'),\n",
    "            # 'LC80albedo': meansLULC.getNumber('80'),\n",
    "            # 'LC90albedo': meansLULC.getNumber('90'),\n",
    "            # 'LC95albedo': meansLULC.getNumber('95'),\n",
    "            # 'LC100albedo': meansLULC.getNumber('100'),\n",
    "            # 'LC10lowAlbedoPct': classFiltAreas.getNumber('10').divide(classAreas.getNumber('10')),\n",
    "            # 'LC20lowAlbedoPct': classFiltAreas.getNumber('20').divide(classAreas.getNumber('20')),\n",
    "            # 'LC30lowAlbedoPct': classFiltAreas.getNumber('30').divide(classAreas.getNumber('30')),\n",
    "            # 'LC40lowAlbedoPct': classFiltAreas.getNumber('40').divide(classAreas.getNumber('40')),\n",
    "            'LC50lowAlbedoPct': classFiltAreas.getNumber('50').divide(classAreas.getNumber('50')),\n",
    "            # 'LC60lowAlbedoPct': classFiltAreas.getNumber('60').divide(classAreas.getNumber('60')),\n",
    "            # 'LC70lowAlbedoPct': classFiltAreas.getNumber('70').divide(classAreas.getNumber('70')),\n",
    "            # 'LC80lowAlbedoPct': classFiltAreas.getNumber('80').divide(classAreas.getNumber('80')),\n",
    "            # 'LC90lowAlbedoPct': classFiltAreas.getNumber('90').divide(classAreas.getNumber('90')),\n",
    "            # 'LC95lowAlbedoPct': classFiltAreas.getNumber('95').divide(classAreas.getNumber('95')),\n",
    "            # 'LC100lowAlbedoPct': classFiltAreas.getNumber('100').divide(classAreas.getNumber('100')),\n",
    "        })\n",
    "\n",
    "    \n",
    "    ## update FeatureCollection with percents\n",
    "    histo = reducers(FC = boundary_geo_ee)\n",
    "    albedo_means=histo.map(count_to_percent).select(['geo_id','LC50lowAlbedoPct'])\n",
    "    \n",
    "    # store in df and append\n",
    "    df = geemap.ee_to_pandas(albedo_means)\n",
    "    df = df.rename(columns={\"LC50lowAlbedoPct\": \"GRE_1_3_percentBuiltwLowAlbedo\"})\n",
    "    cities_indicators_GRE_1_3 = cities_indicators_GRE_1_3.append(df)\n",
    "    \n",
    "    \n",
    "    # process unit of analysis level ------\n",
    "    print(\"\\n boundary_id_unit: \"+boundary_id_unit)\n",
    "    # read boundaries\n",
    "    boundary_path = aws_s3_dir +'/boundaries/v_0/boundary-'+boundary_id_unit+'.geojson'\n",
    "    boundary_geo = requests.get(boundary_path).json()\n",
    "    boundary_geo_ee = geemap.geojson_to_ee(boundary_geo)\n",
    "    \n",
    "    ## update FeatureCollection with percents\n",
    "    histo = reducers(FC = boundary_geo_ee)\n",
    "    albedo_means=histo.map(count_to_percent).select(['geo_id','LC50lowAlbedoPct'])\n",
    "    \n",
    "    # store in df and append\n",
    "    df = geemap.ee_to_pandas(albedo_means)\n",
    "    df = df.rename(columns={\"LC50lowAlbedoPct\": \"GRE_1_3_percentBuiltwLowAlbedo\"})\n",
    "    cities_indicators_GRE_1_3 = cities_indicators_GRE_1_3.append(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e36d60-52d1-4a98-bbed-6271e2a20c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_indicators_GRE_1_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5abd9b-097c-4676-a500-ecd45ad77374",
   "metadata": {},
   "source": [
    "# Merge with indicator table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731fba45-59b3-44fe-94a9-3520763dffed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read indicator table\n",
    "cities_indicators = pd.read_csv(aws_s3_dir + '/indicators/cities_indicators_v2test.csv') \n",
    "cities_indicators#.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978df631-317b-4c42-8c8a-d8ee108e1715",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_indicators(indicator_table, new_indicator_table, indicator_name):\n",
    "    if indicator_name in indicator_table.columns:\n",
    "        print(\"replace with new calculations\")\n",
    "        indicator_table.drop(indicator_name, inplace=True, axis=1)\n",
    "        new_indicator_table = new_indicator_table.drop_duplicates()\n",
    "        cities_indicators_df = indicator_table.merge(new_indicator_table[[\"geo_id\",indicator_name]], \n",
    "                                                     on='geo_id', \n",
    "                                                     how='left',\n",
    "                                                     validate='one_to_many')\n",
    "    else:\n",
    "        print(\"add new indicators\")\n",
    "        new_indicator_table = new_indicator_table.drop_duplicates()\n",
    "        cities_indicators_df = indicator_table.merge(new_indicator_table[[\"geo_id\",indicator_name]], \n",
    "                                                     on='geo_id', \n",
    "                                                     how='left',\n",
    "                                                     validate='one_to_many')\n",
    "    return(cities_indicators_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa73a3a-ea72-42ab-81ed-589b95ef33ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_indicators_merged = merge_indicators(indicator_table = cities_indicators,\n",
    "                                            new_indicator_table = cities_indicators_GRE_1_3,\n",
    "                                            indicator_name = \"GRE_1_3_percentBuiltwLowAlbedo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c10c1e2-9aa9-4d43-a110-8550e4f36e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_indicators_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba258d2-9e13-4fcc-bf22-4aeb3f359405",
   "metadata": {},
   "source": [
    "# Upload in aws s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2953a17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to s3\n",
    "aws_credentials = pd.read_csv('/home/jovyan/PlanetaryComputerExamples/aws_credentials.csv')\n",
    "# aws_credentials = pd.read_csv('C:\\\\Users\\\\Saif.Shabou\\\\OneDrive - World Resources Institute\\\\Documents\\\\aws\\\\credentials.csv')\n",
    "aws_key = aws_credentials.iloc[0]['Access key ID']\n",
    "aws_secret = aws_credentials.iloc[0]['Secret access key']\n",
    "\n",
    "s3 = boto3.resource(\n",
    "    service_name='s3',\n",
    "    aws_access_key_id=aws_key,\n",
    "    aws_secret_access_key=aws_secret\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ef6ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload to aws\n",
    "key_data = 'data/indicators/cities_indicators_v2test.csv'\n",
    "bucket_name = 'cities-cities4forests' \n",
    "cities_indicators_merged.to_csv(\n",
    "    f\"s3://{bucket_name}/{key_data}\",\n",
    "    index=False,\n",
    "    storage_options={\n",
    "        \"key\": aws_key,\n",
    "        \"secret\": aws_secret\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3884339",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make it public\n",
    "object_acl = s3.ObjectAcl(bucket_name,key_data)\n",
    "response = object_acl.put(ACL='public-read')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08d3067-2af3-489b-a7aa-dcb7d95d58f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
